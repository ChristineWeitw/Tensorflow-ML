{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP using RNN_SentimentAnalysis_PlayGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2Y1a+xBp2xy+Wq406YeNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristineWeitw/Tensorflow-ML/blob/master/NLP_using_RNN_SentimentAnalysis_PlayGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdvq6OpHJgeJ",
        "colab_type": "text"
      },
      "source": [
        "# RNN is really good at classification problems & understanding textural data\n",
        "\n",
        "# METHOD [Bag of words] - to convert texture data to numeric data\n",
        "    # they place each words into a Dictionary and parse their corresponding value\n",
        "    # then pack the words tgr by their frequency\n",
        "    # disadvantage : lose the content meaning\n",
        "    # ex : I thought that movie is bad, but it is actually amazing. vs. I thought that movie was amazing, but it is actually bad.add()\n",
        "    \n",
        "# METHOD [Word Embedding]\n",
        "    # A word embedding is a learned representation for text where words that have the same meaning have a similar representation. \n",
        "    # it group the similar meaning words toward close vector; and the oppisite words to an oppsite direction of previous vector.add()\n",
        "    # a vectorized representation of words in a given document that places words with similar meanings near each other.\n",
        "    \n",
        "# RNN cf. CNN & Dense NN\n",
        "    #  CNN & Dense NN are so called \" feed forward NN \",  They process all the data at once\n",
        "    #  RNN on the other hand, has a loop inside its internal model, processes a word per time, then trains it wile passing more data.add()\n",
        "    #  RNN acts like how human read text, read one word at a time and slowly build up its understanding\n",
        "    #  when RNN process a new coming new word, it combines the knowledge that has built from previous words\n",
        "## LSTM (Long Short Term Memory)\n",
        "    # If the text sequence is really long, model will lose the important input info from the beginning text,\n",
        "    # Therefore, LSTM allows us not only look at the current but also keep an eye from the begining text\n",
        "    # LSTM add a component to keep track on the internal states\n",
        "    # by adding LSTM, we can get information from any previous state at any point in the future when we want it now\n",
        "    # instead of only store the previous output, long term memory makes a look up table to let us see any output at any time point.\n",
        "\n",
        "\n",
        "# I. Sentiment analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpvSj5GHkeUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ea89f51a-3e66-4f30-e85c-7e72682c88cf"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAX_Len = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2-ZIYI0bt5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76c46d36-098c-4359-f612-a319f9b2900d"
      },
      "source": [
        "print(len(train_data[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1YM_zF6cJcV",
        "colab_type": "text"
      },
      "source": [
        "## **More Preprocessing**\n",
        "when we look at our loaded in reviews, we will notice that they are diiferent in length. Therefore we must make each review the same length.\n",
        "\n",
        "\n",
        "*   if the review is > 250 words then trim off the extra words\n",
        "*   if the review is < 250 words then we add the necessary amount of 0's to make it equal to 250\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeVfH6yhcCtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data,MAX_Len)\n",
        "test_data = sequence.pad_sequences(test_data, MAX_Len)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC-ss9Y3dabg",
        "colab_type": "text"
      },
      "source": [
        "## **Create the Model**\n",
        "We will use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.\n",
        "\n",
        "32 stands for the output dimension fo the vectors generated by the embedding layer.  We can change this value if we'd like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qktvn1kFdW8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNN_model = tf.keras.Sequential([\n",
        "                                 tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "                                 tf.keras.layers.LSTM(32),\n",
        "                                 tf.keras.layers.Dense(1,activation='sigmoid') # we want the model to generate the output of either 0 or 1 (positive/negative).\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8dFA3xWeomK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d0cab36f-a085-43f8-9d24-c7ef1fed65b4"
      },
      "source": [
        "RNN_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQAayaLWe-nZ",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9660GnSEerAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "265c168a-6834-4e14-8a20-022a2157cbcc"
      },
      "source": [
        "RNN_model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc']) # choose 'binary' since the problem we have is a 2 class problem\n",
        "\n",
        "history = RNN_model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.4195 - acc: 0.8105 - val_loss: 0.2961 - val_acc: 0.8808\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2418 - acc: 0.9087 - val_loss: 0.3838 - val_acc: 0.8510\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1814 - acc: 0.9341 - val_loss: 0.2883 - val_acc: 0.8862\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1493 - acc: 0.9464 - val_loss: 0.2999 - val_acc: 0.8744\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.1282 - acc: 0.9539 - val_loss: 0.2901 - val_acc: 0.8882\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1125 - acc: 0.9607 - val_loss: 0.3726 - val_acc: 0.8870\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0960 - acc: 0.9686 - val_loss: 0.3189 - val_acc: 0.8850\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.0847 - acc: 0.9729 - val_loss: 0.3286 - val_acc: 0.8878\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0755 - acc: 0.9754 - val_loss: 0.4095 - val_acc: 0.8812\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0685 - acc: 0.9781 - val_loss: 0.3992 - val_acc: 0.8894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9B7GBb0fW-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b202f31-0b1e-4304-9f79-6bbe215cdbda"
      },
      "source": [
        "results = RNN_model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.5178 - acc: 0.8518\n",
            "[0.5178341865539551, 0.8518400192260742]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1YAHFkkXvR",
        "colab_type": "text"
      },
      "source": [
        "## **Making Predictions**\n",
        "not let's use our network to make predctions on our own reviews.\n",
        "\n",
        "Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data;."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fK35foXhPep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "11f01498-fc2b-4893-8116-cdddbfa107e8"
      },
      "source": [
        "# make an encoding fucntion to convert the input reviews into proper format that our model can process\n",
        "import keras\n",
        "word_index = imdb.get_word_index() # creates a look up table showing all the word_index of our loaded data\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text) # tokenize the input text\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]  # if the input word is existed in our own word index number, return its word_index; if not, return 0\n",
        "  return sequence.pad_sequences([tokens],MAX_Len)[0]  # pad_sequence only accepts dealing with list, so this is gonna return us a list of a list, and we only want the first value\n",
        "\n",
        "## example\n",
        "text = 'that movie was just amazing, so amazing'\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpgnbCbLl8vA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "33a71f25-5285-4b78-d7da-7a68bb53f23a"
      },
      "source": [
        "# creates a decode fucntion to convert integer back to words.\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_intergers(integers):\n",
        "  PAD = 0\n",
        "  text =''\n",
        "  for num in intergers:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + ''\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_intergers(encoded))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bc32d0afa7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_intergers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-bc32d0afa7d5>\u001b[0m in \u001b[0;36mdecode_intergers\u001b[0;34m(integers)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mPAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintergers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreverse_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'intergers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sax0rCIZQfe5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "690ffb03-e54b-4a35-af28-058c6a8f9391"
      },
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = RNN_model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = 'I love my boyfriend so much. he is such a cutie'\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = 'i hate my boyfriend so much, he is the worst asswhole i haver ever met.'\n",
        "predict(negative_review)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83154446]\n",
            "[0.4324516]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDiy1jCVW-t_",
        "colab_type": "text"
      },
      "source": [
        "## **II. Play Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mPQ5l18Qfq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1cb50b2-cb63-4b1c-bfe3-66a6ff090a11"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'http://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLKCq1QTQfuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXx_Jb_2Xd1t",
        "colab_type": "text"
      },
      "source": [
        "### If you want to RUN YOUR OWN FILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb8SaN8lQfx4",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "14c030de-476c-4129-eed8-eb71721c9478"
      },
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]. ## make sure it is a text file"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b34c4190-f8aa-427b-86dc-c7c73f0747b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b34c4190-f8aa-427b-86dc-c7c73f0747b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f0cf22c78c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     68\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 69\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHkFJ6lPQf1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "131629cc-d3d3-4f55-c26b-2695b6b7f489"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # 'rb' = read bytes\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPar_NCSQf3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "7f93c38e-d867-47ea-8083-334b22f41c32"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-STN0jdIQf9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrfTsYeFYadL",
        "colab_type": "text"
      },
      "source": [
        "### **Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofs0JF29QJGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text)) # figure out how many unique characters are in the data\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):        # this function is just to use the mapping func from above to convert the characters in the text to integers\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "def int_to_text(ints):        # this is just the opposite function, To convert the intergers back to text\n",
        "  try:\n",
        "    ints = ints.numpy() # turn it into numpy array\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_kY9QigaIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "98dd6c27-b5f7-4f2e-cb3e-139670b96740"
      },
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\",text_to_int(text[:13]))\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9xXyh0Kgl8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eaa92152-340e-423c-f7ff-b366a4b7ff86"
      },
      "source": [
        " seq_length = 100 # length of sequence for a training sample\n",
        " examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        " # Create training example / targets\n",
        " char_datasets = tf.data.Dataset.from_tensor_slices(text_as_int) # convert our entire string datasets into characters. And allows us to have a stream of characters (like 1.1 million words)\n",
        " print(char_datasets)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZcXI_SajbJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch it into 101 length\n",
        "sequences = char_datasets.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_x57LIdjnYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we need to use these sequence of length 101 and split them into input and output\n",
        "def split_input_target(chunk): # for teh example hello\n",
        "  input_text = chunk[:-1] # hell\n",
        "  target_text = chunk[:-1] #ello\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target) # apply it to all batches of seuquences"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LczS3nFolOLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "64f960f9-d88b-498e-939f-5b7f5a8637d2"
      },
      "source": [
        "## example\n",
        "for x,y in dataset.take(2):\n",
        "  print('\\n\\nEXAMPLE\\n')\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_axChLamYB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64 # oone batch contain 64 examples\n",
        "VOCAB_SIZE= len(vocab) # number of unique chracter\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequenes, so it doesn't attempt to shuffle the entire sequence in memory.\n",
        "# Instead, it maintains a buffer in which it shuffles elements.\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "# shuffle all the batches and batch it with the proper size\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Jx8l7v5W3O",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7URU8Fze5ZYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "410be7f3-e53c-4ab7-87fc-d5a499e01bfd"
      },
      "source": [
        "print(data)  # 64 examples with 100 characters per example"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlzOoRK3pCqe",
        "colab_type": "text"
      },
      "source": [
        "### **Building the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIQeCsO-o-Q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3537af9b-edcd-4840-cac5-3a1b93ce7596"
      },
      "source": [
        "# this model is gonna take 64 training examples into training and return us 64 output results\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # 'None - is coz we don't know the length of each 64 seqeucnes'\n",
        "                               tf.keras.layers.LSTM(rnn_units,\n",
        "                                                    return_sequences = True,  # So that it keep track of all the intermediate output\n",
        "                                                    stateful=True,\n",
        "                                                    recurrent_initializer = 'glorot_uniform'\n",
        "                                                    ),\n",
        "                               tf.keras.layers.Dense(vocab_size)]) # we want to make the num of output neurons == the vocab_size, and they all add up to 1 probability\\\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM,RNN_UNITS,BATCH_SIZE)\n",
        "model.summary()\n",
        "\n",
        "## 64 is the number of examples, None is the length of the sequences(which we don't know) ;at the end 65 is the VOCAB_SIZE"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERlCk-YdgA0v",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8aaPmygA9w",
        "colab_type": "text"
      },
      "source": [
        "## **Creating a Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D9PGdXhbUEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1c58a14-b7c0-47e2-ca4c-bf6728e3c372"
      },
      "source": [
        "# now we clarify all the dimention/shape first\n",
        "for input_example_batch, target_example_batch in data.take(1):  # ask our model to predict on our firsty batch of training data\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwX-zxr75IjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the example of how our result will look like\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)\n",
        "\n",
        "  ## it will return us 64 results for 64 examples input\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ybMj_It59d3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8c3a7374-1fee-4256-845c-144f0b097ca5"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "  ## for every single training example, we get whatever the length of that training example\n",
        "  ## we have 100 length in each training example, so the model will have 100 time steps,  because RNN feed  only one character each time \n",
        "  ## at every time step, we are actually saving that output as a prediction and parsing that back."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.01046958  0.00570161 -0.00232532 ...  0.00423734 -0.00208422\n",
            "  -0.00077858]\n",
            " [ 0.00615054  0.00605415 -0.00098767 ... -0.00132219  0.00455114\n",
            "   0.00351664]\n",
            " [ 0.00526402  0.00256013 -0.00136691 ... -0.00231734  0.0018963\n",
            "  -0.0005192 ]\n",
            " ...\n",
            " [ 0.00740656  0.00128037 -0.01382843 ...  0.00498741 -0.01445409\n",
            "   0.00687171]\n",
            " [ 0.00915973  0.00107904 -0.01157518 ...  0.00284413 -0.01345574\n",
            "   0.00102549]\n",
            " [ 0.00960581  0.00472066 -0.00960267 ...  0.00750926 -0.00656199\n",
            "   0.00269089]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7V6gQCJ7EUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "20c562a2-626d-4587-8c61-95fb5910fba2"
      },
      "source": [
        "# And finally we will look at a prediction at the first timestep of the first training example\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "\n",
        "  ## now we got a tensor length 65. its 65 values representing the probability of each character occuring next"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 0.01046958  0.00570161 -0.00232532 -0.00743217 -0.01028958  0.00068924\n",
            " -0.00095865 -0.0043709  -0.00642598  0.00162289  0.00367309 -0.00227389\n",
            "  0.00524175 -0.00572277  0.00557022  0.00014716  0.00301498  0.00231852\n",
            "  0.00511751  0.0073759   0.00132013 -0.0007134   0.01477829 -0.00024507\n",
            " -0.00342577 -0.00183641 -0.00235248  0.00195048  0.00363811 -0.00324646\n",
            " -0.0071897  -0.00953974  0.00329146 -0.00159327  0.00534082  0.00217295\n",
            "  0.00053816  0.00463307 -0.00053033  0.00077215 -0.00946033  0.0014554\n",
            "  0.01255078  0.00087957 -0.00013797 -0.00495945  0.00328171 -0.00646829\n",
            " -0.00512948  0.00221696 -0.00572212  0.00312649 -0.00442466 -0.00891806\n",
            " -0.00807228 -0.00463288 -0.00622531  0.01280541 -0.00098653 -0.00167485\n",
            "  0.00164307 -0.00426776  0.00423734 -0.00208422 -0.00077858], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U83Nr3RQ8iY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bce7e3d-a40c-4856-c2b5-05b3ba505875"
      },
      "source": [
        "## in concoustion, the information above show us that we need to make our own loss function\n",
        "## Because RNN do not have a built-in function to interpret such complex result\n",
        "\n",
        "# if we want to determine the predicted character we need to sample the output distribution ( pick a value based on  \"PROBABILITY DISTRIBUTION\" instead of only picking the value with the highest probability value (statistic rule)\n",
        "\n",
        "  ## sampling the 100 time step of 1st training example\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "  ## now we can reshape that array and convert all the intergers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # this is what the model predicted for training sequence 1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"h '.dO3XcdG;?!:mK$vSnttAFB XiYnuga-uovoxQbKWpWjpuu'S$LA,LX'!\\nefRAIUJwhOjmmnUa,XmfFJcoaQ.ho$JpxJ\\nUMrS\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtdofP1MBfMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilize Tensorflow's built in loss function to compute the loss between two things\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Ru0B_DCnim",
        "colab_type": "text"
      },
      "source": [
        "## **Compile the Model**\n",
        "\n",
        "At this point we can think of our problem as a classification problem where the model predicts the probability of each un ique leteer coming next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpQr7wSwClJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2AgH7xbGSNT",
        "colab_type": "text"
      },
      "source": [
        "## **Creating Checkpoints**\n",
        "now we are going to setup and configure our model to save checkpoints as it trans. This will allow us to load our model from a checkpoint and continue training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUvfiCZZGD03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_prefix,\n",
        "      save_weights_only=True\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F38jmsaHE8I",
        "colab_type": "text"
      },
      "source": [
        "## **Training**\n",
        "\n",
        "(checkpoints' meaning?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14KMA5UGmGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3fb2b26-0a90-4630-ced9-0bc96f5571e3"
      },
      "source": [
        "history = model.fit(data,epochs=40,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.8931\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.0028\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 8.7110e-04\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 4.5328e-04\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 2.8520e-04\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 12s 69ms/step - loss: 1.9942e-04\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 12s 69ms/step - loss: 1.4830e-04\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.1390e-04\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 8.9417e-05\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 7.1543e-05\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 5.8769e-05\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 4.8875e-05\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 4.1008e-05\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 3.4774e-05\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 2.9385e-05\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 2.5265e-05\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 2.1489e-05\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.8753e-05\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.6277e-05\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.4349e-05\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.2553e-05\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.1159e-05\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 9.8862e-06\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 8.8448e-06\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 7.9123e-06\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 7.1148e-06\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 6.3375e-06\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 5.6995e-06\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 5.1019e-06\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 4.6207e-06\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 4.1678e-06\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 3.7684e-06\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 3.4099e-06\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 3.0782e-06\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 2.8000e-06\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 2.5421e-06\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 2.3087e-06\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 12s 73ms/step - loss: 2.0974e-06\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.9228e-06\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.7337e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPKYeaOOHj7R",
        "colab_type": "text"
      },
      "source": [
        "In this case, the more epochs you train, more better the result will be.\n",
        "Overfitting problem does not really happen here.\n",
        "\n",
        "## **Load the Model**\n",
        "We will rebuild the model from a checkpoint using a batch_size of 1 so that we can feed onepeice of text to the model and have it make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCqek8dHPuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we change the batch_size to 1\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM,RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95TE7a20Igma",
        "colab_type": "text"
      },
      "source": [
        "Once the model is finished training we an find the LATEST CHECKPOINT that stores the models weights ( the one we trained using 64 batch_size) using following lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhuJOqO-IVXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # out latest checkpoint is at the 40th\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIcDO88qJRxX",
        "colab_type": "text"
      },
      "source": [
        "We can load **any checkpoint** we want by sepecifying the exact file to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKNB5r3IVcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint('./training_checkpoints/ckpt_' +str(checkpoint_num))) # now we load the result from checkpoint 10th\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv7ndLBnJM5T",
        "colab_type": "text"
      },
      "source": [
        "## **Generating Text**\n",
        "### The purpose for this section is to enable us to enter ONE sequence of input then the model can generate the results for us.\n",
        "\n",
        "This lovely fuction is provided by tensorflow to generate some text using any starting string we'd like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKC1b533IVgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step(generating text using the learnt model)\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size ==1\n",
        "  model.reset_state() # when you rebuild a model, it will memorize the latest state. Thus, we have to clean it beforeha nd\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions/ temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9buXynLIVl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = input(\"TYpe a starting string: \")\n",
        "print(generate_Text(model, inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwKxkJ94IVu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYon2S7aIVso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khg9J3HRIVrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kh-Y8UQIVkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKOeignIVa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}